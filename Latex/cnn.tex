\chapter{Redes Neuronales Convolucionales}\label{cnn}

	Las Redes Neuronales Convolucionales o \textit{CNN} son un tipo especializado de red neuronal para el procesamiento de datos con estructura de cuadrícula específica. 
	
	Durante el capítulo describiremos la operación matemática en la que se basan este tipo de redes, conocida como convolución. Posteriormente describiremos otra operación que suelen implementar este tipo de redes, el \textit{pooling}. 
	
	
	\section{La operación de Convolución}
	
	Una convolución es una operacion en dos funciones de argumentos en $\mathbb{R}$ generando una tercera función combinando las dos anteriores. 
	
	Dadas dos funciones que toman valores reales $f, g: \mathbb{R} \rightarrow \mathbb{R}$ con la regularidad necesaria, se denota como $(f \ast g)$ y se define como el producto de ambas funciones habiendo previamente invertido y desplazado una de ellas. Esto es:
	
	$$
		s(t) =	(f \ast g)(t) = \int_{- \infty}^{\infty} f(s) g(t-s) ds
	$$	
	
	En la terminología de las redes neuronales convolucionales, el primer argumento ($f$) es conocido como \textbf{entrada}  y el segundo ($g$) como \textbf{núcleo}. La salida se denomina \textbf{mapa de características}. 
	
	 Para la definición de convolución hemos supuesto $f$ y $g$ funciones continuas definidas en $\mathbb{R}$. Sin embargo, esta condición de proporcionar valores en cada instante de tiempo (de forma continua) no es realista. Cuando trabajamos con datos en un ordenador, el tiempo se discretiza y nuestras funciones pasarían a tomar valores de forma discreta. Para ello, definimos la \textbf{función de convolución discreta,} que será la utilizada por las redes neuronales convolucionales de la siguiente manera:
	 
	 $$
		 s(t) =	(f \ast g)(t) = \sum_{s = - \infty}^{\infty} f(s) g(t-s) 
	 $$
	 
	En los modelos de aprendizaje, la entrada suele ser un vector multidimensional de datos y el núcleo un vector multidimensional de parámetros que se adaptan mediante el algoritmo de aprendizaje. Supondremos que las funciones toman valor 0 en todo su dominio menos en el conjunto finito de puntos que se almacenan como valores. Esto significa que, en la práctica, podemos implementar la suma infinita de la definición como una suma finita de elementos ignorando aquellos elementos que sumen una cantidad nula.
	
	Normalmente se utilizan más de una convolución al mismo tiempo en diferentes ejes. 
	
	Por ejemplo, si consideramos una imagen bidimensional $I$ como entrada, podríamos aplicar un núcleo bidimensional $K$ de la siguiente manera:
	
	
	$$
		S(i,j) = (I \ast K)(i,j) = \sum_{m} \sum_{n} I(m,n)K(i-m, j-n) =  \sum_{m} \sum_{n} K(m,n) I(i-m, j-n)
	$$
	
	\subsection{Ejemplo de convolución}
	
	En esta subsección veremos un ejemplo de convolución sencilla para entender mejor su funcionamiento. 
	
	Por ejemplo, consideremos el último caso estudiado en la sección anterior. Tenemos una imagen bidimensional $I \in \mathcal{M}_{nxm}(\mathbb{R}) = \mathcal{M}_{5x5}(\mathbb{R})$ y aplicamos un filtro $K$ de dimensiones $(i, j) = (2,3)$ en los respectivos ejes. El resultado sería entonces un mapa de características de dimensiones $(n-i+1, m-j+1) = (4, 3)$.
	
	Veamos el funcionamiento de la convolución gráficamente. Tenemos la siguiente entrada:
	
	\begin{center}
		\begin{TAB}(e,1.5cm,1.5cm){|c:c:c:c:c|}{|c:c:c:c:c|}
		
			\textcolor{red}{$I_{(1,1)}$} & \textcolor{red}{$I_{(1,2)}$} & $I_{(1,3)}$& $I_{(1,4)}$&$I_{(1,5)}$\\ 
			\textcolor{red}{$I_{(2,1)}$}& \textcolor{red}{$I_{(2,2)}$}&  $I_{(2,3)}$& $I_{(2,4)}$ &$I_{(2,5)}$\\ 
			\textcolor{red}{$I_{(3,1)}$}&  \textcolor{red}{$I_{(3,2)}$}&  $I_{(3,3)}$&    $I_{(3,4)}$ &    $I_{(3,5)}$ \\ 
			$I_{(4,1)}$&  $I_{(4,2)}$ &  $I_{(4,3)}$&    $I_{(4,4)}$ &    $I_{(4,5)}$ \\
			$I_{(5,1)}$&  $I_{(5,2)}$ &  $I_{(5,3)}$&    $I_{(5,4)}$ &    $I_{(5,5)}$  
		\end{TAB}
	\end{center}

	Tras aplicar la convolución con las dimensiones del núcleo indicadas, denotando a $S$ como la función convolución tenemos:

	\begin{center}
		\begin{TAB}(e,1.5cm,1.5cm){|c:c:c:c|}{|c:c:c|}
		 \textcolor{red}{S(1,1)} & S(1,2) & S(1,3)& S(1,4)\\
		 S(2,1)& S(2,2) & S(2,3)& S(2,4)\\
		 S(3,1)& S(3,2) & S(3,3)& S(3,4)
		\end{TAB}
	\end{center}

	donde $S(1,1) = f(I_{(1,1)}, I_{(1,2)}, I_{(2,1)}, I_{(2,2)}, I_{(3,1)}, I_{(3,2)})$ y así sucesivamente.
	
	\section{\textit{Pooling}}
	
	Una típica capa de una red neuronal consiste en tres pasos:
	
	\begin{enumerate}
		\item La capa realiza varias convoluciones en paralelo para producir un conjunto de activaciones lineales.
		\item Al resultado de cada activación lineal se le aplica una función de activación no lineal. 
		\item Se usa una función de \textit{pooling} para modificar aúm más la salida de la capa.
	\end{enumerate}

	Una función de \textit{pooling} reemplaza la salida de la red en una determinada localización con valores estadísticos de las salidas cercanas. Como ejemplos de funciones de \textit{pooling} podemos destacar el \textbf{\textit{max pooling}} que devuelve el máximo de un vecindario rectangular, o aquella que devuelve la media o la norma $\mathcal{L}^2$ de vecindarios rectangulares.
	
	En todos los casos, el aplicar \textit{pooling} hace la representación prácticamente invariante frente a pequeñas variaciones en la entrada, consiguiendo más capacidad de generalización. 
	
	\section{Adaptación a procesamiento de datos secuencial}	
	
