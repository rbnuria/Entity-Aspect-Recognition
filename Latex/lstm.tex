\chapter{Modelado de secuencias: Redes Neuronales Recurrentes}
Al igual que en el capítulo anterior, en este vamos a desarrollar una clase concreta de redes neuronales, las redes neuronales recursivas y recurrentes. Esta familia de redes neuronales se utilizan para el procesamiento de datos secuencial, por lo que será natural su uso en el problema que se trata en este proyecto. 

\section{Motivación}

La innovación con respecto a las redes neuronales convencionales será el aplicar a estas nuevas técnicas de \textit{deep learning} una de las primeras ideas que surgieron en el aprendizaje automático y los modelos estadísticos: compartir los parámetros a lo largo de todo el aprendizaje. El hecho de compartir parámetros durante el aprendizaje del modelo hará posible la extensión y aplicación del modelo a ejemplos de diferentes longitudes y generalizar a través de ellos. 

Podemos entender la importancia del hecho de compartir parámetros durante el modelo en un ejemplo sencillo de procesamiento del lenguaje natural. Supongamos las frases [1] y [2]:

	\begin{center}
	\begin{minipage}{0.4\linewidth}
		\vspace{5pt}%margen superior de minipage
		{\small
			\textit{[1] Estuve en  Nepal en 2017.}
			\\\textit{[2] En 2017 estuve en Nepal.}
		}
		\vspace{5pt}%margen inferior de la minipage
	\end{minipage}
\end{center}

Si quisiéramos diseñar un algoritmo de aprendizaje automático que reconociera en qué año estuvo el narrador en Nepal, debería de reconocerse el año 2017 como la parte importante de información, independientemente de que aparezca en la quinta o en la segunda palabra de la oración. Una red neuronal tradicional usaría parámetros independientes para cada entrada, por lo que necesitaría aprender todas las reglas del lenguaje de forma independiente para cada posición en la frase. Este problema se solventaría considerando una red neuronal recurrente que comparta los mismos pesos a lo largo de todos los pasos necesarios durante el aprendizaje.

Para simplificar la notación durante el capítulo consideraremos que la red actúa sobre una secuencia que contiene vectores $x^{(t)}$ aunque en la práctica las redes suelen operar sobre conjuntos de ellas (\textit{minibatches}).

Al igual que en capítulo \ref{cnn} se definía la estructura del modelo de las redes neuronales convolucionales como un grafo acíclico, la recurrencia de este tipo de redes ampliará el concepto a grafos cíclicos. Estos ciclos representarán la influencia del valor actual de la variable en su propio valor en el siguiente paso del aprendizaje. 

\section{Despliegue de Grafos Computacionales}

	Un \textbf{grafo computacional } es una forma de formalizar la estructura de un conjunto de cálculos, como aquellos que involucran un conjunto de entradas y parámetros para producir una salida y pérdida.
	
	En esta sección desarrollaremos la idea de \textbf{desplegar} un cálculo recurrente en un grafo computacional con su correspondiente estructura de cadena de eventos. 
	
	Por ejemplo, consideramos la fórmula clásica de un sistema dinámico:
	
	\begin{equation}\label{recursive}
		s^{(t)} = f(s^{(t-1)}; \theta)
	\end{equation}
		
	
	donde $s^{(t)}$ es el estado del sistema.
	
	Es claro que esta fórmula es recurrente porque en la definición de un estado concreto de $s$ en el tiempo $t$ se utiliza el valor del mismo estado en el momento de tiempo $t-1$.
	
	Si consideramos un número de pasos finito $\tau$, el grafo puede ser desplegado aplicando la definición $\tau -1 $ veces.  Por ejemplo, para $\tau = 3$ tenemos:
	
	\begin{equation}\label{recursive2}
		s^{(3)} = f(s^{(2)}; \theta) = f(f(s^{(1)}; \theta); \theta)
	\end{equation}
		
	
	Desplegando sucesivamente la fórmula con la definición de $s$ hemos obtenido una expresión que no involucra recurrencia y que puede ser expresada con un gráfico acíclico. De esta forma el grafo para la fórmula \ref{recursive2} sería:
	
\begin{figure}[h!]
	\centering
	\begin{tikzpicture}[
	init/.style={
		draw,
		circle,
		font = \Large,
		inner sep=2pt,
		join = by -latex
	},
	squa/.style={
		draw,
		inner sep=2pt,
		join = by -latex
	},
	start chain=2,node distance=13mm
	]
	
	\node[on chain=2, init] 
	(s1) {$s^{(1)}$};
	
	\begin{scope}[start chain=3]
		\node[init] at (3, 0cm)  
		(s2) {$s^{(2)}$};
	\end{scope}
	
	\begin{scope}[start chain=4]
	\node[init] at (6, 0cm)  
		(s3) {$s^{(3)}$};
	\end{scope}
	
	\draw[-latex] (s1) -- (s2);
	\draw[-latex] (s2) -- (s3);
	
	\end{tikzpicture}
	\label{fig:recursive2}
\end{figure}

\vspace{0.5cm}

	Y de forma genérica, para la fórmula \ref{recursive} tendríamos el siguiente grafo
	
\vspace{0.5cm}

\begin{figure}[h!]

	\centering
\begin{tikzpicture}[
init/.style={
	draw,
	circle,
	minimum size=1.3cm,
	inner sep=2pt,
	join = by -latex
},
squa/.style={
	draw,
	inner sep=2pt,
	join = by -latex
},
start chain=2,node distance=13mm
]

\node[on chain=2, init] 
(s1) {$s^{(...)}$};

\begin{scope}[start chain=3]
\node[init] at (3, 0cm)  
(s2) {$s^{(t-1)}$	};
\end{scope}

\begin{scope}[start chain=4]
\node[init] at (6, 0cm)  
(s3) {$s^{(t)}$};
\end{scope}

\begin{scope}[start chain=5]
\node[init] at (9, 0cm)  
(s4) {$s^{(t+1)}$};
\end{scope}

\begin{scope}[start chain=6]
\node[init] at (12, 0cm)  
(s5) {$s^{(...)}$};
\end{scope}

\draw[-latex, dashed] (s1) -- (s2);
\draw[-latex] (s2) -- (s3);
\draw[-latex] (s3) -- (s4);
\draw[-latex, dashed] (s4) -- (s5);

\end{tikzpicture}
\label{fig:recursive}
\end{figure}

\vspace{0.5cm}

La mayoría de las redes neuronales recurrentes utilizan la fórmula \ref{hidden} para definir el valor de salida de las capas ocultas. 

\begin{equation}\label{hidden}
	h^{(t)} = f(h^{(t-1)}, x^{(t)}; \theta)
\end{equation} 
	
Cuando una red neuronal recurrente se entrena para una tarea que requiere predecir un resultado a partir de unos datos, aprende usando $h^{(t)}$ como un tipo de resumen de pérdida de los aspectos relevantes para la tarea de la secuencia de entradas hasta el momento $t$. 

\vspace{0.5cm}

La ecuación \ref{hidden} se puede representar de dos formas diferentes:
\begin{enumerate}
	\item Considerando un nodo por cada uno de los estados por los que pasa cada una de las variables del modelo como en la Figura \ref{fig:forma1}.
	
	\begin{figure}[h!]
		\centering
		\begin{tikzpicture}[
		init/.style={
			draw,
			circle,
			minimum size=1.3cm,
			inner sep=2pt,
			join = by -latex
		},
		squa/.style={
			draw,
			inner sep=2pt,
			join = by -latex
		},
		start chain=2,node distance=13mm
		]
		
		\node[on chain=2, init] 
		(s1) {$h^{(...)}$};
		
		\begin{scope}[start chain=3]
		\node[init] at (3, 0cm)  
		(s2) {$h^{(t-1)}$};
		\node[init] at (3, -2cm)  
		(x2) {$x^{(t-1)}$};
		\end{scope}
		
		\begin{scope}[start chain=4]
		\node[init] at (6, 0cm)  
		(s3) {$h^{(t)}$};
		
		\node[init] at (6, -2cm)  
		(x3) {$x^{(t)}$};
		\end{scope}
		
		\begin{scope}[start chain=5]
		\node[init] at (9, 0cm)  
		(s4) {$h^{(t+1)}$};
		
		\node[init] at (9, -2cm)  
		(x4) {$x^{(t+1)}$};
		\end{scope}
		
		\begin{scope}[start chain=6]
		\node[init] at (12, 0cm)  
		(s5) {$h^{(...)}$};
		\end{scope}
		
		\draw[-latex, dashed] (s1) -- (s2);
		\draw[-latex] (s2) -- (s3);
		\draw[-latex] (s3) -- (s4);
		\draw[-latex, dashed] (s4) -- (s5);
		\draw[-latex] (x2) -- (s2);
		\draw[-latex] (x3) -- (s3);
		\draw[-latex] (x4) -- (s4);
		
		\end{tikzpicture}
		\caption{Representación extendida de las operaciones de las capas ocultas en una red neuronal recursiva.}
		\label{fig:forma1}
	\end{figure}

	\item Considerando una forma resumida, donde cada nodo representa las componentes que existirían en una implementación física del modelo como en la Figura \ref{fig:forma2}. 

	
	\begin{figure}[h!]
		\centering
		\begin{tikzpicture}[
		init/.style={
			draw,
			circle,
			pin edge={loop,thin,black},
			minimum size=1.3cm,
			inner sep=2pt,
			join = by -latex
		},
		nit/.style={
			pin edge={loop,thin,black},
			minimum size=1.3cm,
			inner sep=2pt,
			join = by -latex
		},
		squa/.style={
			draw,
			inner sep=2pt,
			join = by -latex
		},
		start chain=2,node distance=13mm
		]
		
		\node[on chain=2, init, pin = {[nit]}, label={[label distance=1.3cm]above:f}] 
		(h) {\textbf{h}};
		
		\begin{scope}[start chain=3]
		\node[init] at (0, -2cm)  
		(x) {\textbf{x}};
		\end{scope}
		
		
		\draw[-latex] (x) -- (h);

		
		\end{tikzpicture}
		\caption{Representación esquematizada de las operaciones de las capas ocultas en una red neuronal recursiva.}
		\label{fig:forma2}
	\end{figure}

\end{enumerate}
	
	Ambas formas de representación tienen sus ventajas, mientras que la segunda es compacta, la primera nos ofrece una descripción explícita de los cálculos que se llevarán a cabo. A lo largo del capítulo utilizaremos indiferentemente ambas notaciones. 
	
	\section{Redes Neuronales Recurrentes}
	
	Una vez hemos introducido la idea de compartir parámetros durante todo el aprendizaje y el despliegue de grafos computacionales podemos diseñar una gran variedad de redes neuronales recurrentes. 
	
	Podemos observar un esquema de red recurrente en la Figura  \ref{fig:rnrgenerica}.
	
		\begin{figure}[h!]

		\begin{tikzpicture}[
		init/.style={
			draw,
			circle,
			pin edge={loop,thin,black},
			minimum size=1.3cm,
			inner sep=2pt,
		},
		nit/.style={
			pin edge={loop,thin,black,},
			minimum size=1.3cm,
			inner sep=2pt,
			join = by -latex
		},
		squa/.style={
			draw,
			inner sep=2pt,
			join = by -latex
		},
		start chain=2,node distance=13mm
		]
		
		
		
		\node[on chain=2, init] 
		(y) {\textbf{y}};
		
		\node[on chain=2, init]  at (5, 0cm)  
		(y2) {$y^{(t-1)}$};
		
		\node[on chain=2, init] at (7, 0cm)  
		(y3) {$y^{(t)}$};
		
		\node[on chain=2, init] at (9, 0cm)  
		(y4){$y^{(t+1)}$};
		
		\begin{scope}[start chain=3]
		\node[init] at (0, -2cm)  
		(L) {\textbf{L}};
		
		\node[init] at (6.95, -2cm)  
		(L2) {$L^{(t-1)}$};
		
		\node[init] at (8.95, -2cm)  
		(L3) {$L^{(t)}$};
		
		\node[init] at (10.95, -2cm)  
		(L4) {$L^{(t+1)}$};
		\end{scope}
		
		\begin{scope}[start chain=4]
		\node[init] at (0, -4cm)  
		(o) {\textbf{o}};
		
		\node[init] at (6.95, -4cm)  
		(o2) {$o^{(t-1)}$};
		
		\node[init] at (8.95, -4cm)  
		(o3) {$o^{(t)}$};
		
		\node[init] at (10.95, -4cm)  
		(o4) {$o^{(t+1)}$};
		\end{scope}
		
		\begin{scope}[start chain=5]
		\node[init,  label={[label distance=1cm]right:\textbf{W}}, label = {[label distance = 0.1cm]75:\textbf{V}}] at (0, -6cm)  
		(h) {\textbf{h}};
		
		\node[init, label = {[label distance = 0.01cm]15:\textbf{W}}] at (4.95, -6cm)  
		(h2) {$h^{(...)}$};
		
		\node[init,  label = {[label distance = 0.01cm]15:\textbf{W}}, label = {[label distance = 0.1cm]75:\textbf{V}}] at (6.95, -6cm)  
		(h3) {$h^{(t-1)}$};
		
		\node[init,  label = {[label distance = 0.01cm]15:\textbf{W}}, label = {[label distance = 0.1cm]75:\textbf{V}}] at (8.95, -6cm)  
		(h4) {$h^{(t)}$};
		
		\node[init,  label = {[label distance = 0.01cm]15:\textbf{W}}, label = {[label distance = 0.1cm]75:\textbf{V}}] at (10.95, -6cm)  
		(h5) {$h^{(t+1)}$};
		
		\node[init,  label = {[label distance = 0.01cm]15:\textbf{W}}] at (12.95, -6cm)  
		(h6) {$h^{(...)}$};
		
		\end{scope}
		
		\begin{scope}[start chain=6]
		\node[init,  label = {[label distance = 0.1cm]75:\textbf{U}}] at (0, -8cm)  
		(x) {\textbf{x}};
		
		\node[init,  label = {[label distance = 0.1cm]75:\textbf{U}}] at (6.95, -8cm)  
		(x2) {$x^{(t-1)}$};
		
		\node[init,  label = {[label distance = 0.1cm]75:\textbf{U}}] at (8.95, -8cm)  
		(x3) {$x^{(t)}$};
		
		\node[init,  label = {[label distance = 0.1cm]75:\textbf{U}}] at (10.95, -8cm)  
		(x4) {$x^{(t+1)}$};
		\end{scope}
		
		
		\draw[-latex] (y) -- (L);
		\draw[-latex] (o) -- (L);
		\draw[-latex] (h) -- (o);
		\draw[-latex] (x) -- (h);

		
		\draw[-latex] (y2) -- (L2);
		\draw[-latex] (o2) -- (L2);
		\draw[-latex] (h3) -- (o2);
		\draw[-latex] (x2) -- (h3);

		
		\draw[-latex] (y3) -- (L3);
		\draw[-latex] (o3) -- (L3);
		\draw[-latex] (h4) -- (o3);
		\draw[-latex] (x3) -- (h4);

		
		\draw[-latex] (y4) -- (L4);
		\draw[-latex] (o4) -- (L4);
		\draw[-latex] (h5) -- (o4);
		\draw[-latex] (x4) -- (h5);
		
		\draw[-latex, dashed] (h2) -- (h3);
		\draw[-latex] (h3) -- (h4);
		\draw[-latex] (h4) -- (h5);
		\draw[-latex, dashed] (h5) -- (h6);
		
		\draw[-latex] (h) to [out=20,in=340,looseness=8](h);
				
		\end{tikzpicture}
		\caption{Representaciones de una red neuronal recurrente genérica.}
		\label{fig:rnrgenerica}
	\end{figure}

	En cuanto a la notación, establecemos como \textbf{x}  los valores de entrada y \textbf{o} con la secuencia de valores de salida. Por su parte, una medida \textbf{L} determina cómo de acertado es cada valor de \textbf{o} para la correspondiente etiqueta de entrenamiento \textbf{y}. La entrada de las capas ocultas están parametrizadas por una matriz de pesos \textbf{U}, mientras que las conexiones entre capas ocultas recurrentes se parametrizan con la matriz de pesos \textbf{W} compartida, y la conexión de las capas ocultas con la salida está parametrizada por la matriz de pesos \textbf{V}. 
	
	Una vez establecida la arquitectura de la red, nos centramos en desarrollar las ecuaciones propagación hacia delante (\textit{forward propagation}).  En este caso supondremos que la función de activación de las capas ocultas será una tangente hiperbólica (\textit{tanh}) y que la salida es discreta. Una forma común de representar variables discretas es considerar la salida \textbf{o} como una probabilidad logarítmica no normalizada para cada posible valor de la variable discreta. Entonces, podemos aplicar una operación \textit{softmax} a dichas probabilidades como se explica en la sección \ref{softmax}.
	
	Con todas estas consideraciones, tomando $t \in [0, \tau]$ podemos definir las siguientes ecuaciones:
	
	\begin{equation}\label{1}
		\textbf{a}^{(t)} = \textbf{b} + \textbf{W h}^{(t-1)} +\textbf{ Ux}^{(t)}
	\end{equation} 
	
	\begin{equation}	
		\textbf{h}^{(t)} = \tanh(\textbf{a}^{(t)})
	\end{equation}
	
	\begin{equation}
		\textbf{o}^{(t)} = \textbf{c} + \textbf{Vh}^{(t)}
	\end{equation}
	
	\begin{equation}\label{2}
		\hat{\textbf{{y}}}^{(t)} = softmax(\textbf{o}^{(t)} )
	\end{equation}
	
	donde los parámetros son los vectores de sesgo \textbf{b} y \textbf{c} junto con las matrices de pesos anteriormente comentadas \textbf{U}, \textbf{V} y \textbf{W}. 
	
	En cuanto a la función de pérdida para una secuencia de entrada \textbf{x} emparejada con una secuencia de valores \textbf{y} sería simplemente la suma de las pérdidas. Por ejemplo, considerando la entropía cruzada $\mathcal{L}^{(t)}$ de $y^{(t)}$ dados $\textbf{x}^{(t)}, ..., \textbf{x}^{(t)}$ entonces tenemos:
	
	\begin{equation}
			\mathcal{L}(\{ \textbf{x}^{(1)}, ..., \textbf{x}^{(\tau)} \}, \{ \textbf{y}^{(1)}, ..., \textbf{y}^{(\tau)} \}) = \sum_{t}\mathcal{L}^{(t)} ) = 
			- \sum_{t} \log p(y^{(t)} | \textbf{x}^{(1)}, ..., \textbf{x}^{(t)} \})
	\end{equation}
	
	Calcular el gradiente de esta función de pérdida con respecto a los parámetros es una operación costosa. Su cálculo require de una propagación hacia delante de izquierda a derecha seguida de una propagación hacia atrás de derecha a izquierda en nuestro grafo. El tiempo de ejecución sería $\mathcal{O}(\tau)$ y no se puede reducir con paralización porque el grafo es inherentemente secuencial, cada paso de tiempo tiene que ser ejecutado después de haber ejecutado el anterior. Los cálculos realizados en la propagación hacia delante se almacenan utilizándose en la propagación hacia atrás, así conseguimos una coste de memoria de $\mathcal{O}(\tau)$ nuevamente.  
	
	El algoritmo de propagación hacia atrás (\textit{backpropagation}) utilizado recorrer el grafo en $\mathclap{O}(\tau)$ se llama \textbf{\textit{back-propagation through time} (BPTT)} o retro-propagación a través del tiempo y lo estudiamos en la siguiente sección.
	
	\subsection{Cálculo del Gradiente}
	 
	El cálculo del gradiente en una red neuronal recurrente es sencillo, basta con aplicar el algoritmo \textit{back-propagation} clásico al gráfico extendido. 
	
	Para entender mejor el comportamiento del método \textit{BPTT}, lo aplicaremos sobre las ecuaciones \ref{1} a la \ref{2}:
	
	Para cada nodo \textbf{N} necesitamos calcular el gradiente $\nabla_{\textbf{N}}L$ recursivamente basándonos en el gradiente calculado en los nodos que lo siguen en el grafo.
	
	Empezamos la recurrencia con los nodos que siguen inmediantamente a la pérdida final:
	
	\begin{equation}
		\frac{\partial L}{ \partial L^{(t)}} = 1
	\end{equation}
	
	Asumiendo que las salidas $\textbf{o}^{(t)}$ se usan como argumento para una función \textit{softmax} que nos proporciona el vector de probabilidades $\hat{\textbf{y}}$ sobre la salida, el gradiente $\nabla_{\textbf{o}^{(t)}}L$ de las salidas en el paso de tiempo $t$, para todo $i,t$ es:
	
	\begin{equation}
		(\nabla_{\textbf{o}^{(t)}}L)_i = \frac{\partial L}{\partial o_i{(t)}} = \frac{\partial L}{\partial L^{(t)}}\frac{\partial L^{(t)}}{\partial o_i{(t)}} = \hat{y}_i^{(t)} - \textbf{1}_{i,y^{(t)}}
	\end{equation}
	
	Como estamos cálculando hacia atrás, empezando por el final de la secuencia. En el paso final $\tau$, $\textbf{h}^{\tau}$ solo tendría $\textbf{o}^{\tau}$ como descendiente, por lo que su gradiente es sencillo:
	
	\begin{equation}
		\nabla_{\textbf{h}^{(\tau)}}L = \textbf{V}^T \nabla_{\textbf{o}^{(\tau)}}L
	\end{equation}
	
	Iterando ahora de forma descendente desde $t  = \tau - 1$ hasta $t = 1$, observamos que $\textbf{h}^{(t)}$ tendría como descendientes a $\textbf{o}^{(t)}$ y  $\textbf{h}^{(t-1)}$. Entonces, el gradiente se calcula de la siguiente manera:
	
	\begin{equation}
		\nabla_{\textbf{h}^{(t)}}L = \left( \frac{\partial \textbf{h}^{(t+1)}}{\partial \textbf{h}^{(t)}}  \right)^T (\nabla_{\textbf{h}^{(t+1)}}L) + \left(\frac{\partial \textbf{o}^{(t)}}{\partial \textbf{h}^{(t)}} \right)^T(\nabla_{\textbf{o}^{(t)}}L) =
	\end{equation}
	
	$$
		= \textbf{W}^T (\nabla_{\textbf{h}^{(t+1)}}L) diag\left(1 - (\textbf{h}^{(t+1)})^2\right) + \textbf{V}^T(\nabla_{\textbf{o}^{(t)}}L)
	$$
	
	Donde diag$\left(1 - (\textbf{h}^{(t+1)})^2\right) $ se corresponde con la matriz diagonal cuyos elementos son $1 - ( \textbf{h}_i^{(t+1)})^2$ que es la matriz Jacobiana de la tangente hiperbólica asociada a la unidad oculta $i$ en el momento de tiempo $t+1$.
	
	Una vez que hemos obtenido el gradiente de los nodos internos del grafo de cómputo, podemos obtener los gradientes en los nodos de parámetros. A la hora de calcular el gradiente de esas variables tenemos que tener en cuenta que los parámetros se comparten a lo largo de los pasos. Usando $\nabla_{\textbf{W}^{(t)}}$ para notar la contribución de los pesos en el instante de tiempo $t$ al gradiente, las expresiones de los gradientes del resto de parámetros son:
	
	\begin{equation}
		\nabla_{\textbf{c}}L = \sum_{t}\left( \frac{\partial \textbf{o}^{(t)}}{\partial \textbf{c}} \right)^T \nabla_{\textbf{o}^{(t)}}L = \sum_{t} \nabla_{\textbf{o}^{(t)}}L
	\end{equation}
	
	\begin{equation}
	\nabla_{\textbf{b}}L = \sum_{t}\left( \frac{\partial \textbf{h}^{(t)}}{\partial \textbf{b}^{(t)}} \right)^T \nabla_{\textbf{h}^{(t)}}L = \sum_{t} diag\left(1 - \left(\textbf{h}^{(t)}\right)^2\right)    \nabla_{\textbf{h}^{(t)}}L
	\end{equation}
	
	\begin{equation}
	\nabla_{\textbf{v}}L = \sum_{t} \sum_{i} \left( \frac{\partial L}{\partial o_o^{(t)}}\right) \nabla_{\textbf{v}}{o_i^{(t)}} = \sum_{t} (\nabla_{\textbf{o}^{(t)}}L) \textbf{h}^{(t)^T}
	\end{equation}
	
	\begin{equation}
		\nabla_{\textbf{W}}L = \sum_{t} \sum_{i} \left(\frac{\partial L}{ \partial h_i^{(t)}}\right) \nabla_{\textbf{W}^{(t)}} h_i^{(t)} = \sum_{t}diag\left( 1 - \left(\textbf{h}^{(t)}\right)^2\right) (\nabla_{\textbf{h}^{(t)}}L)\textbf{h}^{(t-1)^T}
	\end{equation}
	
	\begin{equation}
		\nabla_{\textbf{U}}L = \sum_{t} \sum_{i} \left(\frac{\partial L}{\partial h_i^{(t)}} \right) \nabla_{\textbf{U}^{(t)}}h_i^{(t)} = \sum_t diag \left(1 - \left(h^(t)\right)^2\right) (\nabla_{\textbf{h}^{t}}L)\textbf{x}^{(t)^T}
	\end{equation}
	
%	\subsection{Redes Neuronales Recurrentes como Modelos de Grafos Dirigidos}
%	
%	En el caso anteriormente desarrollado, las pérdidas $L^{(t)}$ eran la entropía cruzada entre las etiquetas de entrenamiento $\textbf{y}^{(t)}$ y las salidas $\textbf{o}^{(t)}$. 
%	
%	Cuando usamos como función objetivo la una probabilidad predictiva, como el estimador de máxima verosimilitud, entrenamos a nuestro modelo para que estime la distribución condicional del siguiente elemento de la secuencia $\textbf{y}^{(t)}$ dadas las entradas anteriores. Esto es, queremos minimizar
%	
%	\begin{equation}
%		\log p(\textbf{y}^{(t)}|\textbf{x}^{(1)}, ..., \textbf{x}^{(t)}),
%	\end{equation}
%	
%	o, en los modelos que  incluyen conexiones desde las salidas de un paso al siguiente paso,
%	
%	\begin{equation}
%		\log p(\textbf{y}^{(t)}|\textbf{x}^{(1)}, ..., \textbf{x}^{(t)}, \textbf{y}^{(1)}, ..., \textbf{y}^{(t)}),
%	\end{equation}

	%\subsection{Modelando Secuencias condicionadas al Contexto}
	
	\subsection{El problema de las dependencias a largo plazo}\label{dependenciaslargoplazo}
	
	El problema de aprender dependencias a largo plazo en las redes recurrentes radica en que el gradiente propagado durante muchas etapas tiende a desaparecer (la mayoría de las veces) o a explotar. Incluso si asumimos que los parámetros del modelo son aquellos que hacen a la red estable, la dificultad de las dependencias a largo plazo surgen de los pesos exponencialmente más pequeños dados en las iteracciones a largo plazo (involucrando multiplizaciones de varias matrices Jacobianas) en comparación con las de corto plazo. Durante esta sección expondremos el problema con más precisión.
	
	Las redes recurrentes involucran la composición de la misma función múltiples veces, una por paso. Estas composiciones pueden provocar un comportamiento extremadamente alejado de lo lineal. En particular, la composición de funciones empleada en las redes neuronales recurrentes son similares a la multiplicación de matrices. Podemos pensar en la relación recurrente 
	
	\begin{equation}
		\textbf{h}^{(t)} = \textbf{W}^T\textbf{h}^{(t-1)}
	\end{equation}
	
	como una red neuronal recurrente muy simple sin función de activación y sin entradas. Podemos simplificar esta expresión de la forma
	
	\begin{equation}
		\textbf{h}^{(t)} = (\textbf{W}^{t})^T\textbf{h}^{(0)},
	\end{equation}
	
	y si $\textbf{W}$ admite descomposición en valores propios de la forma
	
	\begin{equation}
		\textbf{W} = \textbf{Q} \Lambda \textbf{Q}^T
	\end{equation}
	
	con $\textbf{Q}$ matriz ortogonal, la recurrencia podría ser simplificada aún más
	
	\begin{equation}
		\textbf{h}^{(t)} = \textbf{Q}^T \Lambda^t \textbf{Q}\textbf{h}^{(0)}
	\end{equation}
	
	Los valores propios se elevan a la potencia $t$, produciendo valores propios con magnitud menos de uno para tender desvanecerse y valores propios con magnitud mayor que uno para explotar. Así, toda componente de $\textbf{h}^{(0)}$ que no esté alineada con el mayor valor propio tenderá a ser descartada.
	
	\subsection{Redes neuronales recurrentes con memoria a corto-largo plazo}
	
	Las redes neuronales recurrentes con memoria a corto-largo plazo o \textit{long short-term memory (LSTM)} tratan de solucionar el problema de las dependencias a largo plazo \ref{dependenciaslargoplazo}. 
	
	Estas redes son el ejemplo canónico de un subgrupo de redes neuronales, las \textbf{redes neuronales recurrentes con puertas} o \textit{\textbf{gated RNNs}}. Se basan en la idea de crear caminos a través del tiempo que tengan derivadas que no se desvanezcan ni exploten. 
	
	Las \textit{LSTM} surgen de la idea de introducir auto-bucles para producir caminos donde el gradiente puede fluir durante largos períodos de tiempo. Además, introduen la innovación de que los pesos del auto-bucle estén condicionados al contexto. Al controlar los pesos del auto-bucle a través otra  unidad oculta, la escala de tiempo de integración puede cambiar dinámicamente. Es decir, incluso para una \textit{LSTM} de parámetros fijos, la escala de tiempo de integración podría cambiar en función de la secuencia de entrada. Las \textit{LSTM} han producido resultados exitosos en muchas aplicaciones, entre ellas el Procesamiento del Lenguaje Natural (PLN), motivo por el que las desarrollamos durante esta memoria.
	
		
	\begin{figure}[h!]
		
		\begin{tikzpicture}[
		init/.style={
			shade,
			circle,
			pin edge={loop,thin,black},
			minimum size=1.3cm,
			inner sep=2pt,
		},
		init2/.style={
			draw,
			circle,
			pin edge={loop,thin,black},
			minimum size=1.3cm,
			inner sep=2pt,
		},
		squa/.style={
			draw,
			inner sep=2pt,
			join = by -latex
		},
		start chain=2,node distance=13mm
		]
		
		
		
		\node[on chain=2, init, label = {[label distance = 0.01cm]below:input}] 
		(i1) {$\backsim$};
		
		\node[on chain=2, init, label = {[label distance = 0.01cm]below:input gate}]  at (2, 0cm)  
		(i2) {$\backsim$};
		
		\node[on chain=2, init, label = {[label distance = 0.01cm]below:forget gate}] at (6, 0cm)  
		(i3) {$\backsim$};
		
		\node[on chain=2, init, label = {[label distance = 0.01cm]below:output gate}] at (10, 0cm)  
		(i4){$\backsim$};
		
		\begin{scope}[start chain=3]
			\node[on chain=3, init2] at (0, 3cm)
			(i5) {$\times$};
			
		\end{scope}
		
		\begin{scope}[start chain=3]
			\node[on chain=3, init, label = {[label distance = 0.01cm]350:state} , label = {[label distance = 1cm]45:self-loop}] at (0, 6cm)
			(i6) {$+$};
			
			\node[on chain=3, init2] at (2, 6cm)
			(i8) {$\times$};
		\end{scope}
		
		\begin{scope}[start chain=3]
			\node[on chain=3, init, label = {[label distance = 0.5cm]80:output}] at (0, 9cm)
			(i7) {$+$};
		\end{scope}
		

		\draw[-latex] (i1) -- (i5);
		\draw[-latex] (i2) -- (i5);
		\draw[-latex] (i3) -- (i8);
		\draw[-latex] (i3) -- (i8);
		\draw[-latex] (i5) -- (i6);
		\draw[-latex] (i8) -- (i6);
		\draw[-latex] (i6) -- (i7);
		\path[every node/.style={font=\sffamily\small}]
			(i4) edge[-latex, bend right] node [left] {} (i7);
		\path[every node/.style={font=\sffamily\small}]
			(i6) edge[-latex, bend right = -90] node [left] {} (i8);
		\path[every node/.style={font=\sffamily\small}]
			(i6) edge[-latex, bend right = 90] node [left] {} (i1);
		\path[every node/.style={font=\sffamily\small}]
			(i6) edge[-latex, bend right = 90] node [left] {} (i2);
		\path[every node/.style={font=\sffamily\small}]
			(i6) edge[-latex, bend right = 20] node [left] {} (i3);
		\path[every node/.style={font=\sffamily\small}]
			(i6) edge[-latex, bend right = 20] node [left] {} (i4);
			
		
		\end{tikzpicture}
		\caption{Representaciones de una red neuronal recurrente genérica.}
		\label{fig:celllstm}
	\end{figure}
	
	En la Figura \ref{fig:celllstm} se esquematiza el diagrama de bloque que representa una celda de una \textit{LSTM}. Las celdas se conectan entre ellas recurrentemente, reemplazando las unidades ocultas de las redes neuronales comunes. Estas celdas tienen una recurrencia interna (\textit{self-loop}) además de la recurrencia interna entre celdas. Cada celda tiene las mismas entradas y salidas que una red recurrente normal, pero tiene más parámetros y sistemas de puertas que controlan el flujo de información. 
	
	La componente más importante de esta estructura es el estado $s_i^{(t)}$, el cual tiene una auto-bucle lineal. Los pesos de este auto-bucle están determinados por una unidad de olvido o \textit{forget gate unit} $f_i^{(t)}$, la cual fija los pesos a un valor en $[0,1]$ mediante una unidad sigmoidal:
	
	\begin{equation}
		f_i^{(t)}=\sigma \left( b_i^f  + \sum_{j}U_{i,j}^f x_j^{(t)} + \sum_{j} W_{i,j}^f h_j^{(t-1)}\right),
	\end{equation}

	donde $\textbf{x}^{(t)}$ es el actual vector de entrada y $\textbf{h}^{(t)}$ es el vector de la actual capa oculta, que contiene las salidas de todas las celdas previas y $\textbf{b}^{(t)}$ , $\textbf{U}^{(t)}$  y $\textbf{W}^{(t)}$ son los sesgos, pesos de entrada y pesos recurrentes de las unidades de olvido respectivamente. 
	
	El valor del estado interno de la celda se actualiza mediante la siguiente fórmula:
	
	\begin{equation}
		s_i^{(t)} = f_i^{(t)} s_i^{(t-1)} + g_i^{(t)} \sigma \left( b_i+ \sum_{j}U_{i,j} x_j^{(t)} + \sum_{j} W_{i,j} h_j^{(t-1)}\right)
	\end{equation} 
	
	La unidad de entrada externa o \textit{external input gate unit} $g_i^{(t)}$ se calcula de forma similar a la unidad de olvido (mediante una unidad sigmoidal) pero con sus propios parámetros:
	
	\begin{equation}
	g_i^{(t)} =  \sigma \left( b_i^g+ \sum_{j}U_{i,j}^g x_j^{(t)} + \sum_{j} W_{i,j}^g h_j^{(t-1)}\right)
	\end{equation} 
	
	La salida $h_i^{(t)}$ de la celda se puede expresar en función de la unidad de salida $q_i^{(t)}$ que también utiliza una unidad sigmoidal:
	
	\begin{equation}
		h_i^{(t)} = \tanh \left(s_i^{(t)} \right) q_i^{(t)},
	\end{equation}
	
	\begin{equation}
	q_i^{(t)} =  \sigma \left( b_i^o+ \sum_{j}U_{i,j}^o x_j^{(t)} + \sum_{j} W_{i,j}^o h_j^{(t-1)}\right)
	\end{equation} 
	
	cuyos parámetros $\textbf{b}^o$,   $\textbf{U}^o$ y  $\textbf{W}^o$ son los sesgos, pesos de entrada y pesos recurrentes respectivamente. 

	\vspace{1cm}

	Las redes neuronales a corto-largo plazo han mostrado aprender las dependencias a largo plazo más fácilmente que las arquitecturas recurrentes dado que solucionan su principal inconveniente, por lo que serán las que se utilizarán en la práctica.