\chapter{Análisis de los resultados}
En este capítulo se realizará un análisis de los resultados obtenidos a partir de los experimentos descritos en el Capítulo \ref{preparacionexperimentos}. En la Sección \ref{tools} se analizará el comportamiento de las herramientas de manera individual, así como de los ensamblados de estas. En la Sección \ref{evo} veremos los resultados del aprendizaje de pesos mediante metaheurísticas para las herramientas para optimizar el comportamiento del ensamblado de estas.
%TODO poner aquí la sección de lso experimentos de algoritmos genéticos

\section{Análisis de las herramientas} \label{tools}
En esta sección primero haremos un análisis del funcionamiento de las herramientas de forma individual (sección \ref{individual}) y posteriormente veremos el comportamiento del ensamblado de las herramientas.

\subsection{Herramientas individualmente} \label{individual}
En esta sección analizaremos en el comportamiento de las herramientas de forma individual sobre las bases de datos. Primero se realizará un análisis general de todas las herramientas y después analizaremos el comportamiento en conjuntos de datos con determinadas particularidades.

\subsubsection{Análisis general}
Una de las medidas bases para realizar el análisis son las tasas de error de cada herramienta para cada conjunto de comentarios, después de realizar las normalizaciones y el etiquetado que se ha descrito en las secciones anteriores. A continuación una tabla con los errores de cada herramienta:
 
\begin{table}[H] 
	\scalebox{0.8}{
	\begin{tabular}{|llllb{1.5cm}b{1.5cm}cc|}  
		\hline 
		\rowcolor{lightgray}
		Dataset &	Azure&	bing	&	coreNLP	&
		Meaning Cloud
		& 
		Senti Strength
		&
		Syuzhet
		&
		VADER
		\\ \hline
		aisopos\_ntua
		& 
		\cellcolor{greentable} 0.368
		& 
		0.494
		& 
		0.522
		& 
		0.414
		& 
		0.39
		& 
		0.554
		& 
		0.374
		\\ \hline
		debate
		& 
		0.544
		& 
		0.53
		& 
		0.585
		& 
		\cellcolor{greentable} 0.528
		& 
		0.564
		& 
		0.566
		& 
		0.557
		\\ \hline
		english\_dailabor
		& 
		0.413
		& 
		0.358
		& 
		0.577
		& 
		0.374
		& 
		\cellcolor{greentable} 0.313
		& 
		0.529
		& 
		0.335
		\\ \hline
		nikolaos\_ted
		& 
		0.439
		& 
		0.488
		& 
		\cellcolor{greentable} 0.383
		& 
		0.405
		& 
		0.487
		& 
		0.444
		& 
		0.507
		\\ \hline
		pang\_movie
		& 
		0.392
		& 
		 0.491
		& 
		\cellcolor{greentable} 0.265
		& 
		0.480
		& 
		0.566
		& 
		0.401
		& 
		0.573
		\\ \hline
		sanders
		& 
		0.447
		& 
		0.425
		& 
		0.592
		& 
		0.447
		& 
		 0.419
		& 
		0.553
		& 
		\cellcolor{greentable} 0.397
		\\ \hline
		setistrength\_bbc
		& 
		0.689
		& 
		 0.45
		& 
		0.57
		& 
		0.553
		& 
		\cellcolor{greentable} 0.415
		& 
		0.495
		& 
		0.531
		\\ \hline
		sentistrength\_digg
		& 
		\cellcolor{greentable} 0.457
		& 
		0.493
		& 
		0.522
		& 
		0.473
		& 
		\cellcolor{greentable} 0.457
		& 
		0.485
		& 
		0.515
		\\ \hline
		sentistrength\_myspace
		& 
		0.366
		& 
		0.47
		& 
		0.686
		& 
		0.376
		& 
		\cellcolor{greentable} 0.283
		& 
		0.363
		& 
		 0.340
		\\ \hline
		sentistrength\_rw
		& 
		0.465
		& 
		0.482
		& 
		0.638
		& 
		0.438
		& 
		0.456
		& 
		0.461
		& 
		\cellcolor{greentable} 0.434
		\\ \hline
		sentistrength\_twitter
		& 
		0.419
		& 
		0.44
		& 
		0.591
		& 
		0.423
		& 
		\cellcolor{greentable} 0.391
		& 
		0.489
		& 
		0.417
		\\ \hline
		sentistrength\_youtube
		& 
		0.399
		& 
		0.419
		& 
		0.566
		& 
		\cellcolor{greentable} 0.388
		& 
		0.364
		& 
		0.406
		& 
		 0.391
		\\ \hline
		stanford\_tweets
		& 
		\cellcolor{greentable} 0.267
		& 
		0.398
		& 
		0.381
		& 
		0.364
		& 
		0.373
		& 
		0.298
		& 
		0.353
		\\ \hline
		tweet\_semevaltest
		& 
		0.516
		& 
		0.405
		& 
		0.632
		& 
		0.400
		& 
		0.388
		& 
		0.493
		& 
		\cellcolor{greentable} 0.375
		\\ \hline
		vader\_amazon
		& 
		0.453
		& 
		0.487
		& 
		0.434
		& 
		0.451
		& 
		0.56
		& 
		\cellcolor{greentable} 0.419
		& 
		0.536
		\\ \hline
		vader\_movie
		& 
		0.416
		& 
		0.483
		& 
		\cellcolor{greentable} 0.231
		& 
		0.469
		& 
		0.560
		& 
		0.395
		& 
		0.565
		\\ \hline
		vader\_nyt
		& 
		0.515
		& 
		0.558
		& 
		0.467
		& 
		0.510
		& 
		0.592
		& 
		\cellcolor{greentable} 0.439
		& 
		0.624
		\\ \hline
		vader\_twitter
		& 
		0.222
		& 
		0.357
		& 
		0.712
		& 
		0.282
		& 
		0.264
		& 
		0.234
		& 
		\cellcolor{greentable} 0.178
		\\ \hline
		Error medio & 0.432 & 0.457 & 0.519 & 0.449 & 0.435 & 0.446 & 0.445 \\ \hline
\end{tabular} } 
	\caption{Tabla con las tasas de error de clasificación de cada herramienta para cada base de datos}
\label{TABLA_ERRORES}
\end{table}

Viendo los resultados vemos cómo ninguna herramienta ofrece una tasa de acierto excesivamente buena. En verde están remarcadas las mejores herramientas para cada conjunto de datos. Para observar mejor la calidad de las herramientas, en la Tabla \ref{tablaranking} tenemos un resumen en el que se puede ver el ránking de cada herramienta para cada conjunto de comentarios según el error de clasificación.

\newpage

\begin{table} [H]

	\scalebox{0.8}{
	\begin{tabular}{|llllb{1.5cm}b{1.5cm}cc|}
		\hline 
		\rowcolor{lightgray}
		Dataset &	Azure&	bing	&	coreNLP	&
		Meaning Cloud
		& 
		Senti Strength
		&
		Syuzhet
		&
		VADER
		\\ \hline
		aisopos\_ntua & 1 & 5 & 6 & 4 & 3 & 7 & 2
		\\ \hline
		debate & 3 & 2 & 7 & 1 & 5 & 6 & 4
		\\ \hline
		english\_dailabor & 5 & 3 & 7 & 4 & 1 & 6 & 2
		\\ \hline
		nikolaos\_ted & 2 & 5 & 1 & 4 & 6 & 3 & 7
		\\ \hline
		pang\_movie & 2 & 5 & 1 & 4 & 6 & 3 & 7
		\\ \hline
		sanders & 4* & 3 & 7 & 4* & 2 & 6 & 1
		\\ \hline
		sentistrength\_bbc & 7 & 2 & 6 & 5 & 1 & 3 & 4
		\\ \hline
		sentistrength\_digg & 1* & 5 & 7 & 3 & 1* & 4 & 6
		\\ \hline
		sentistrength\_myspace & 4 & 6 & 7 & 5 & 1 & 3 & 2
		\\ \hline
		sentistrength\_rw & 5 & 6 & 7 & 2 & 3 & 4 & 1
		\\ \hline
		sentistrength\_twitter & 3 & 5 & 7 & 4 & 1 & 6 & 2
		\\ \hline
		sentistrength\_youtube & 4 & 6 & 7 & 3 & 1 & 5 & 2
		\\ \hline
		stanford\_tweets & 1 & 7 & 6 & 4 & 5 & 2 & 3
		\\ \hline
		tweet\_semevaltest & 6 & 4 & 7 & 3 & 2 & 5 & 1
		\\ \hline
		vader\_amazon & 4 & 5 & 2 & 3 & 7 & 1 & 6
		\\ \hline
		vader\_movie & 3 & 5 & 1 & 4 & 6 & 2 & 7
		\\ \hline
		vader\_nyt & 4 & 5 & 2 & 3 & 6 & 1 & 7
		\\ \hline
		vader\_twitter & 2 & 6 & 7 & 5 & 4 & 3 & 1
		\\ \hline
		Ránking medio & 3.39 & 4.72 & 5.28 & 3.61 & 3.39 & 3.89 & 3.61
		\\ \hline
	\end{tabular}
}
\caption{Tabla con los ránkings de cada herramienta en cada conjunto de comentarios}
	\label{tablaranking}
\end{table}

Como se puede observar, los SAMs con mejor ránking medio son Azure y SentiStrength, empatando, seguida por MeaningCloud y VADER. Otras herramientas, como CoreNLP, presenta clasificaciones peores. Estos malos resultados también podría deberse a cómo se ha elegido interpretar los resultados, ya que hay varias formas de interpretarlos aunque para los siguientes experimentos es conveniente realizarlos así. Sin embargo, es concretamente CoreNLP la que tiene unos resultados muy buenos en comentarios críticos de películas, precisamente el tipo de comentarios sobre los que la herramienta ha sido entrenada. MeaningCloud muestra casi siempre buenos resultados en comparación con sus competidoras, por lo que aparentemente es la que mejor se adapta al contexto de los textos.

Si nos fijamos en todas las bases de datos con \textit{tweets}, vemos como VADER tiene unas clasificaciones y resultados buenos. Como ya se ha mencionado previamente, VADER tiene un ajuste especial para funcionar bien en \textit{tweets} y queda patente en los resultados de los experimentos realizados.

El conjunto de comentarios que peores resultados tiene es sentistrength\_bbc. Es posible que esto se deba a que es el conjunto de datos que mayor media de frases tiene por comentario, ya que el sentimiento global de un texto es más fácil de detectar en textos pequeños que en textos con gran contenido de frases, debido a que en los textos largos es muy probable que exista mucha mezcla de polaridades, siendo incluso para un ser humano difícil detectar una polaridad global. Otra de los motivos de estos malos resultados es el hecho comentado previamente de que son comentarios en un foro que dependen uno de otro. Esto también ocurría con vader\_nyt y sentistrength\_digg, que concretamente también tienen errores muy altos.

Vemos como curiosamente en el conjunto de datos vader\_twitter, todas las herramientas obtienen una buena tasa de error y, sin embargo, CoreNLP obtiene su peor resultado, siendo además abultadamente malo.

En cuanto a los errores medios, Azure sigue siendo la mejor, esta vez en solitario, sin embargo por ejemplo MeaningCloud obtiene un error medio más alto (unas milésimas solamente) que otras herramientas que hemos considerado como peores según el ránking medio. Este error medio puede verse influenciado al mal comportamiento de ciertas herramientas en determinados contextos, como por ejemplo Azure que obtiene resultados más o menos aceptables en todas a excepción de sentistrength\_bbc donde obtiene un error muy alto.

Otro aspecto a tener en cuenta es qué tendencia tienen las herramientas a la hora de predecir, es decir, si predicen más comentarios positivos, negativos o neutros. En la siguiente tabla (Tabla \ref{table:numpos}) se hace un resumen de cuantos comentarios ha predicho cada herramienta para cada clase. En ella se puede observar como casi todas las herramientas tienen cierta tendencia a predecir positivo, como por ejemplo Azure, pudiéndose explicar así por qué en sentistrength\_bbc obtiene tan malos resultados, ya que este conjunto de comentarios tiene cierto desbalanceo habiendo pocos comentarios positivos. CoreNLP, sin embargo, tiene más dificultad para predecir como positivo.

\newpage
%En vader\_nyt también vemos errores muy altos.
\begin{table} [H]
	\scalebox{0.8}{
	\begin{tabular}{|cccccccc|}
	\hline
	 & Azure & Bing & CoreNLP & MeaningCloud & SentiStrength & Syuzhet & VADER  \\ \hline
	 Negativos & 19405 & 16094 & 27782 & 16573 & 17029 & 18082 & 10962 \\ \hline
	 Neutros & 14696 & 23846 & 22774 & 19714 & 23711 & 10599 & 26308 \\ \hline
	 Positivos & 30295 & 24456 & 13840 & 28109 & 23656 & 35715 & 27126 \\ \hline
\end{tabular}
}
\caption{Número de comentarios positivos, negativos y neutros predichos por cada herramienta}
\label{table:numpos}
\end{table}

Para el análisis de las herramientas resulta útil observar cómo se distribuyen las AUCs y los errores en las distintas herramientas y en los distintos conjuntos de comentarios. En las Figuras \ref{fig:boxplottoolsauc} y \ref{fig:errorsboxplottools} vemos las distribuciones de las AUCs y los errores respectivamente según las herramientas. Por otro lado, en las Figuras \ref{fig:aucsdatasets} y \ref{fig:grafica} encontramos boxplots que representan lo mismo pero según los conjuntos de comentarios.

\begin{table} [H]
	\scalebox{0.8}{
		\begin{tabular}{|llllb{1.5cm}b{1.5cm}cc|}
			\hline 
			\rowcolor{lightgray}
			Dataset &	Azure&	bing	&	coreNLP	&
			Meaning Cloud
			& 
			Senti Strength
			&
			Syuzhet
			&
			VADER
			\\ \hline
			aisopos\_ntua & 0.771 & 0.664  &  \cellcolor{greentable}0.804  &0.732 & 0.757 & 0.633 & 0.774 \\ \hline
			debate & 0.630 & 0.651 &  0.651 & \cellcolor{greentable} 0.655 & 0.604 & 0.612 & 0.660 \\ \hline
			english\_dailabor & 0.701 & 0.720 & 0.699 & 0.720 & 0.732 & 0.666 & \cellcolor{greentable} 0.747 \\ \hline
			nikolaos\_ted & 0.650 & 0.660 & 0.696 & \cellcolor{greentable} 0.699 & 0.670  & 0.627 & 0.694 \\ \hline
			pang\_movie & 0.615 & 0.627  & \cellcolor{greentable} 0.731 & 0.618 & 0.594 & 0.605  & 0.630 \\ \hline
			sanders & 0.644 & 0.662 & 0.646 & 0.656 & 0.667 & 0.631 &  \cellcolor{greentable}0.679 \\ \hline
			sentistrength\_bbc & 0.553 & 0.642 & \cellcolor{greentable} 0.677 & 0.608 & 0.648 & 0.639 & 0.618 \\ \hline
			sentistrength\_digg & 0.645 & 0.663 & \cellcolor{greentable} 0.698 & 0.667 & 0.678 & 0.640 & 0.676 \\ \hline
			sentistrength\_myspace & 0.671 & 0.672 & 0.619 & 0.695 &  \cellcolor{greentable}0.732 & 0.700 & 0.715 \\ \hline
			sentistrength\_rw &0.668 & 0.679 & 0.679 & 0.705 & 0.668 & 0.679 &  \cellcolor{greentable}0.707 \\ \hline
			sentistrength\_twitter &0.704 & 0.703 & 0.658 & 0.718 & \cellcolor{greentable} 0.734 & 0.667  & 0.725 \\ \hline
			sentistrength\_youtube &0.710 & 0.735 & 0.705 & 0.746 & 0.753 & 0.710 &  \cellcolor{greentable}0.759 \\ \hline
			stanford\_tweets & 0.734 & 0.749 & 0.663 & 0.701 & 0.742 & 0.701 & \cellcolor{greentable} 0.755 \\ \hline
			tweet\_semevaltest & 0.669 & 0.699 & 0.661 & 0.706 & 0.700 & 0.652 &  \cellcolor{greentable}0.730 \\ \hline
			vader\_amazon & 0.650 & 0.682 & 0.666 & 0.694 & 0.666 & 0.639 &  \cellcolor{greentable}0.706 \\ \hline
			vader\_movie & 0.618 & 0.634  \cellcolor{greentable}& 0.748 & 0.626 & 0.603 & 0.607 & 0.632 \\ \hline
			vader\_nyt & 0.646 & 0.668 & 0.631 & 0.661 & 0.671 & 0.642 &  \cellcolor{greentable}0.684 \\ \hline
			vader\_twitter & 0.750 & 0.774 & 0.665 & 0.775 & 0.781 & 0.755 & \cellcolor{greentable} 0.829 \\ \hline
			AUC promedio & 0.669 & 0.682 & 0.683 & 0.688 & 0.674 & 0.656 & \cellcolor{greentable} 0.707 \\ \hline
		\end{tabular}
	}
	
	
	\caption{Tabla de las AUCs de cada herramienta respecto a cada base de datos}
\end{table}

Se observa como curiosamente, a pesar de que Azure tiene menor tasa de error medio, tiene la segunda peor AUC media. También CoreNLP en aisopos\_ntua, donde tiene una tasa de error de las más altas, posee una AUC muy alta, lo que es bastante extraño. Al mirar la matriz de confusión se observa cómo no es muy significativa, ya que se realiza la AUC media para cada una de las clases y para la clase negativa solo predice un comentario como negativo, el cuál acierta, por lo que para la clase negativa el AUC sería 1. Es interesante pues observar otras medidas como el \textit{recall} o la precisión \footnote{Para los \textit{datasets} pang\_movie y stanford\_tweets no hay comentarios neutros, por lo que para el recall se ha hecho el recall medio teniendo en cuenta solo 2 clases y para la precisión media la precisión de la clase neutra es 0.}. Si observamos en las Tablas \ref{table:recalls} y \ref{table:precission}, vemos respectivamente los recalls y precisión medios de cada algoritmo. Estas tablas están más detalladamente explicadas en los anexos del proyecto.

En estas Tablas (\ref{table:recalls} y \ref{table:precission}) se puede apreciar cómo el AUC no es el único dato que debemos tener en cuenta. Azure obtiene una de las AUCs más bajas. Sin embargo, en las siguientes tablas se muestra como Azure es la que tiene el segundo Recall más alto (aunque tiene una de las precisiones más bajas). Por lo tanto, los resultados de las AUCs se pueden ver influenciados por el desbalanceo en algunas bases de datos o por el hecho de que sea las AUCs medias debido a que hay 3 clases. Además, sobre la particularidad comentada anteriormente de CoreNLP con aisopos\_ntua, en la tabla completa de la precisión que se encuentra en los anexos, se observa como la precisión para la clase negativa de CoreNLP es 0.008, por lo que contrasta y contradice el AUC tan alto que recibe.

Otras particularidades que se pueden observar en los anexos son la baja precisión a la hora de la predicción de comentarios neutros que muestran las herramientas en conjuntos de comentarios como vader\_twitter o vader\_nyt, probablemente debiéndose esto al desbalanceo existente en estos \textit{datasets}.
\newpage
\begin{table} [H]
	\scalebox{0.8}{
		\begin{tabular}{|llllb{1.5cm}b{1.5cm}cc|}
			\hline 
			\rowcolor{lightgray}
			Dataset &	Azure&	bing	&	coreNLP	&
			Meaning Cloud
			& 
			Senti Strength
			&
			Syuzhet
			&
			VADER
			\\ \hline
			aisopos\_ntua & \cellcolor{greentable} 0.695 & 0.496 & 0.425 & 0.614 & 0.628 & 0.468 & 0.639 \\ \hline
			debate & 0.461 & 0.472 & 0.377 & \cellcolor{greentable} 0.488 & 0.432 & 0.46 & 0.449\\ \hline
			english\_dailabor & 0.689 & 0.634 & 0.4 & 0.678 & \cellcolor{greentable} 0.699 & 0.594 & 0.664 \\ \hline
			nikolaos\_ted & 0.526 & 0.515 & 0.493 & \cellcolor{greentable} 0.555 & 0.518 & 0.475 & 0.506 \\ \hline
			pang\_movie & 0.607 & 0.508  & \cellcolor{greentable} 0.735 & 0.52 & 0.433 & 0.599  & 0.427 \\ \hline
			sanders &0.576 & 0.556 & 0.354 & 0.577 & \cellcolor{greentable} 0.578 & 0.542 & 0.57 \\ \hline
			sentistrength\_bbc & 0.414 & 0.545 & 0.417 & 0.49 & \cellcolor{greentable} 0.562 & 0.512 & 0.501\\ \hline
			sentistrength\_digg &0.535 & 0.523 & 0.453 & 0.548 & \cellcolor{greentable}0.549 & 0.532 & 0.524 \\ \hline
			sentistrength\_myspace & 0.558 & 0.499 & 0.428 & 0.567 & 0.559 & 0.564 & \cellcolor{greentable} 0.57 \\ \hline
			sentistrength\_rw &0.515 & 0.478 & 0.393 & \cellcolor{greentable} 0.517 & 0.513 & 0.477 & 0.495 \\ \hline
			sentistrength\_twitter & \cellcolor{greentable}0.606 & 0.544 & 0.391 & 0.579 & 0.599 & 0.534 & 0.554 \\ \hline
			sentistrength\_youtube &0.568 & 0.556 & 0.455 & 0.575 & \cellcolor{greentable} 0.588 & 0.549 & 0.565 \\ \hline
			stanford\_tweets & \cellcolor{greentable} 0.773 & 0.601 & 0.623 & 0.635 & 0.627 & 0.701 & 0.645 \\ \hline
			tweet\_semevaltest & 0.5 & 0.575 & 0.377 & \cellcolor{greentable} 0.608 & 0.602 & 0.546 & 0.591\\ \hline
			vader\_amazon & 0.579 & \cellcolor{greentable} 0.58 & 0.551 & 0.575 & 0.545 & 0.543 & 0.557 \\ \hline
			vader\_movie & 0.462 & 0.453 & \cellcolor{greentable} 0.568 & 0.454 & 0.41 & 0.423 & 0.408 \\ \hline
			vader\_nyt &  \cellcolor{greentable}0.545 & 0.529 & 0.465 & 0.543 & 0.518 & 0.525 & 0.512 \\ \hline
			vader\_twitter &0.687 & 0.767 & 0.472 & 0.812 & 0.734 & 0.759 & \cellcolor{greentable} 0.867 \\ \hline
			Recall promedio &  0.572 & 0.545 & 0.465 &\cellcolor{greentable} 0.574 & 0.566 & 0.544 & 0.558 \\ \hline
		\end{tabular}
	}
	\caption{Tabla de las Recalls de cada herramienta respecto a cada base de datos}
	\label{table:recalls}
\end{table}

\begin{table} [H]
	\scalebox{0.8}{
		\begin{tabular}{|llllb{1.5cm}b{1.5cm}cc|}
			\hline 
			\rowcolor{lightgray}
			Dataset &	Azure&	bing	&	coreNLP	&
			Meaning Cloud
			& 
			Senti Strength
			&
			Syuzhet
			&
			VADER
			\\ \hline
			aisopos\_ntua &  \cellcolor{greentable}0.68 & 0.496 & 0.651 & 0.597 & 0.606 & 0.46 & 0.642\\ \hline
			debate &0.458 & 0.479 & 0.465 & \cellcolor{greentable} 0.482 & 0.437 & 0.457 & 0.479\\ \hline
			english\_dailabor &0.579 & 0.577 & 0.482 & 0.592 & \cellcolor{greentable} 0.621 & 0.521 & 0.619\\ \hline
			nikolaos\_ted & 0.512 & 0.507 & 0.52 &  \cellcolor{greentable} 0.548 & 0.517 & 0.49 & 0.526 \\ \hline
			pang\_movie & 0.449 & 0.461 &  \cellcolor{greentable}0.565 & 0.452 & 0.428 & 0.438 & 0.463 \\ \hline
			sanders &0.515 & 0.507 & 0.428 & 0.516 & 0.529 & 0.5 & \cellcolor{greentable} 0.539 \\ \hline
			sentistrength\_bbc & 0.378 & 0.488 & 0.498 & 0.437 &  \cellcolor{greentable}0.501 & 0.498 & 0.464\\ \hline
			sentistrength\_digg &0.52 & 0.51 & 0.525 & 0.526 &  \cellcolor{greentable}0.529 & 0.516 & 0.522 \\ \hline
			sentistrength\_myspace & 0.515 & 0.48 & 0.451 & 0.535 &  \cellcolor{greentable}0.597 & 0.528 & 0.579 \\ \hline
			sentistrength\_rw &0.494 & 0.487 & 0.496 & 0.533 & 0.509 & 0.509 &  \cellcolor{greentable}0.535 \\ \hline
			sentistrength\_twitter &0.586 & 0.552 & 0.466 & 0.575 &  \cellcolor{greentable}0.607 & 0.527 & 0.587\\ \hline
			sentistrength\_youtube &0.561 & 0.571 & 0.507 & 0.584 & \cellcolor{greentable} 0.598 & 0.555 & 0.596 \\ \hline
			stanford\_tweets &0.568 & 0.583 & 0.497 & 0.535 & 0.576 & 0.535 &  \cellcolor{greentable}0.589 \\ \hline
			tweet\_semevaltest &0.485 & 0.555 & 0.441 & 0.576 & 0.574 & 0.524 & \cellcolor{greentable} 0.592\\ \hline
			vader\_amazon & 0.497 & 0.525 & 0.508 & 0.536 & 0.507 & 0.484 & \cellcolor{greentable} 0.548 \\ \hline
			vader\_movie & 0.452 & 0.468 &  \cellcolor{greentable}0.581 & 0.46 & 0.437 & 0.441 & 0.466 \\ \hline
			vader\_nyt & 0.495 & 0.512 & 0.48 & 0.507 & 0.515 & 0.492 &  \cellcolor{greentable}0.527 \\ \hline
			vader\_twitter & 0.584 & 0.608 & 0.499 & 0.61 & 0.616 & 0.59 & \cellcolor{greentable} 0.663 \\ \hline
			Precisión promedio & 0.518 & 0.52 & 0.503 & 0.533 & 0.539 & 0.504 &\cellcolor{greentable} 0.552 \\ \hline
		\end{tabular}
	}
	\caption{Tabla de la precisión de cada herramienta respecto a cada base de datos}
	\label{table:precission}
\end{table}

\newpage
\begin{figure} [H]
	\centering
	\includegraphics[width=0.7\linewidth]{imagenes/boxplotToolsAUC}
	\caption{Boxplots de las AUCs de las herramientas}
	\label{fig:boxplottoolsauc}
\end{figure}

\begin{figure} [H]
	\centering
	\includegraphics[width=0.7\linewidth]{imagenes/errorsboxplottools}
	\caption{Boxplots de los errores de las herramientas}
	\label{fig:errorsboxplottools}
\end{figure}

Observamos varias particularidades en las gráficas de las Figuras \ref{fig:boxplottoolsauc} y \ref{fig:errorsboxplottools}. Vemos como Azure que es la que tiene unos errores más bajos obtiene, sin embargo, unas AUCs más bajas. 

En los boxplots organizados según el conjunto de datos se observan por ejemplo errores como el de debate que son parecidos y además muy altos. Este conjunto de datos se trata de \textit{tweets} de debate político. El alto error podría deberse a la cantidad de faltas de ortografías que hay en los comentarios o al hecho de que los \textit{tweets} consten de un máximo de 140 carácteres y los usuarios tengan que acortar los comentarios. Un ejemplo de instancia con errores ortográficos es \textit{'McCain -2 gving credit to house republicans \#tweetdebate
'}. Como este último tweet hay muchos más y además otros con tonos irónicos muy difíciles de detectar.

Como se observa tanto en los boxplots como en las tablas de errores, vader\_twitter es el conjunto de datos que mejores resultados nos da, teniendo unas AUCs bastantes altas y distribuidas en un intervalo pequeño y unas tasas de error relativamente bajas (a excepción de CoreNLP).
\begin{figure} [H]
	\centering
	\includegraphics[width=0.7\linewidth]{imagenes/AUCSdatasets}
	\caption{Boxplots de las AUCs en los distintos datasets}
	\label{fig:aucsdatasets}
\end{figure}

\begin{figure} [H]
	\centering
	\includegraphics[width=0.7\linewidth]{imagenes/grafica}
	\caption{Boxplots de los errores en los distintos datasets}
	\label{fig:grafica}
\end{figure}


%TODO aquí vamos a analizar cómo se comportan las herramientas en las bases de datos más desbalanceadas
%\subsubsection{Comportamiento en conjuntos de datos con cierto desbalanceo}
%Con el fin de comprobar mejor la tendencia de las herramientas a la hora de la detección del sentimiento, se analizarán las matrices de confusión de ciertos conjuntos de datos que presentan particularidades. Analizaremos el comportamiento en:
%\begin{itemize}
%	\item \textbf{sentistrength\_bbc} debido a que es donde menos porcentaje de comentarios positivos encontramos.
%	\item \textbf{vader\_twitter} ya que posee muy pocos comentarios clasificados como neutros.
%	\item \textbf{sentistrength\_myspace} por poseer pocos comentarios clasificados como negativos.
%\end{itemize}

%El conjunto de datos \textbf{sentistrength\_bbc} vemos como se trata del dataset con más media de frases por comentario. Este hecho puede suponer una mayor dificultad de encontrar un sentimiento global en un comentario, ya que cada frase podría expresar distintos sentimientos e incluso por una persona el sentimiento global del comentario podría estar sometido a cierta subjetividad del humano.

%Se observa además como las tasas de error son altas y las áreas bajo la curva bajas en general, destacando los resultados de Azure, que obtiene sus peores resultados en este dataset.

\subsection{Ensamblado de herramientas}
En la Tabla \ref{table:combinaciones} se muestran las tasas de error de clasificación que se ha conseguido con los dos operadores de ensamblado descritos previamente y se comparan con el mejor error de clasificación de los obtenidos por las herramientas individualmente.
\begin{table} [H]
	\scalebox{0.7}{
		\begin{tabular}{|c|ccc|}
			\hline 
			\textbf{Dataset} & \textbf{Polaridad media} &\textbf{Combinación proneutral}  & \textbf{Mejor error individual}\\ 
			\hline 
			aisopos\_ntua &0.378 &0.394 & \cellcolor{greentable}0.368\\ \hline
			debate &\cellcolor{greentable} 0.522  & 0.529  & 0.528\\ \hline
			english\_dailabor & 0.243  & \cellcolor{greentable}0.226 & 0.313\\ \hline
			nikolaos\_ted & 0.522 & 0.549 &\cellcolor{greentable} 0.383\\ \hline
			pang\_movie & 0.586 & 0.626 &\cellcolor{greentable} 0.265\\ \hline
			sanders & 0.321 &\cellcolor{greentable} 0.308 & 0.397\\ \hline
			sentistrength\_bbc & 0.573 &0.573 &\cellcolor{greentable} 0.415\\ \hline
			sentistrength\_digg & 0.498 &0.518 &\cellcolor{greentable} 0.457\\ \hline
			sentistrength\_myspace & 0.418 & 0.446 &\cellcolor{greentable} 0.283\\ \hline
			sentistrength\_rw &\cellcolor{greentable} 0.428 & 0.455 & 0.434\\ \hline
			sentistrength\_twitter &\cellcolor{greentable} 0.386 & 0.395 & 0.391\\ \hline
			sentistrength\_youtube &\cellcolor{greentable} 0.382 & 0.402 & 0.388\\ \hline
			stanford\_tweets & 0.459 & 0.499 &\cellcolor{greentable} 0.267\\ \hline
			tweet\_semevaltest &\cellcolor{greentable} 0.338 & 0.340 & 0.375\\ \hline
			vader\_amazon & 0.615 & 0.643 &\cellcolor{greentable} 0.419\\ \hline
			vader\_movie & 0.585 & 0.624 &\cellcolor{greentable} 0.231\\ \hline
			vader\_twitter & 0.366 & 0.435 &\cellcolor{greentable} 0.178\\ \hline
			vader\_nyt & 0.687& 0.719 &\cellcolor{greentable} 0.439\\ \hline
		\end{tabular} 
	}
	\caption{Tabla resumen de los errores medios de las combinaciones con las que vamos a comparar los resultados comparados con el error mínimo en las herramientas individualmente}
	\label{table:combinaciones}
\end{table}

En la Tabla \ref{table:combinaciones} se aprecia como los ensamblados no obtienen unos resultados demasiado buenos. Sin embargo, en hasta 7 conjuntos de datos obtenemos mejores resultados que los conseguidos con las herramientas individualmente. De hecho, en dos de ellos (english\_dailabor y sanders) se obtiene una tasa de error de casi un 10\% menor. En otros conjuntos de comentarios, como por ejemplo en stanford\_tweets o vader\_movie se obtienen unos resultados muy malos en comparación a las herramientas de forma individual. Como hemos visto anteriormente, estos conjuntos de datos, entre otros, tienen buenos resultados con alguna herramienta en particular y muy malos con el resto, muy probablemente debido al contexto de los comentarios y al contexto en el que las herramientas han sido entrenadas o configuradas.

El hecho de que se consigan errores tan altos en \textit{datasets} en los que hay alguna herramienta concreta que destaque se debe lógicamente a que estamos teniendo en cuenta otras herramientas que seguramente introduzcan ruido debido a su mala calidad en estos conjuntos de comentarios determinados. Este hecho es una justificación más por la que puede resultar útil buscar los pesos mediante metaheurísticas. Si nuestras hipótesis no fallan, los algoritmos evolutivos desarrollados deberían adaptar los pesos de manera que en un conjunto de datos donde una herramienta concreta funciona mejor, debería asignar más peso a esta herramienta que al resto.
\section{Algoritmos evolutivos} \label{evo}
En esta sección analizaremos primero los errores de clasificación obtenidos una vez hemos asignado los pesos a las herramientas con los algoritmos evolutivos (sección \ref{algorresults}). Posteriormente veremos y analizaremos el comportamiento de los algoritmos evolutivos a la hora de buscar la combinación de pesos más óptima posible.

\subsection{Resultados algoritmos evolutivos} \label{algorresults}
En esta sección veremos cómo ha influido la búsqueda de pesos en las tasas de error medias. Compararemos con los 'ensembles' anteriormente descritos y con las herramientas individualmente, ya que el error medio de ambas opciones sería igual si realizáramos las particiones como hemos hecho para el aprendizaje de pesos.

Las primeras ejecuciones se realizaron sobre todos los conjuntos de datos (es decir, todos los \textit{datasets} en un mismo fichero), con el fin de encontrar una combinación de pesos que optimizara de forma general el ensamblado de las distintas herramientas. Sin embargo, los pobres resultados obtenidos (mostrados tanto en la tabla \ref{table:evolutivos} como en la \ref{table:mejoras}) invitan a pensar el hecho de que buscar unos pesos de forma genérica puede ser contraproducente ya que, como también se ha propuesto anteriormente, cada herramienta se comporta de forma diferente depende del contexto (véase el ejemplo de CoreNLP como el más evidente). En las tablas se marca en verde cuál ha sido el algoritmo que mejor resultado ha dado en cada \textit{dataset}.
\newpage
\begin{table} [H]
	\scalebox{0.8}{
	\begin{tabular}{|l|llll|}
		\hline 
	\textbf{Dataset} & \textbf{AGG-CA} &\textbf{ AGE-CA} & \textbf{DE-Current-to-best} & \textbf{DE-Rand} \\ 
		\hline 
		aisopos\_ntua & 0.3380 & 0.3340 & 0.3260 & 0.3260\\ \hline
		debate & 0.5312 &\cellcolor{greentable} 0.5213 & 0.5235 & 0.5232 \\ \hline
		english\_dailabor & 0.2397 &0.2334 & 0.2259 & 0.2262\\ \hline
		nikolaos\_ted & 0.5172 & 0.5173& 0.4827 & 0.4827 \\ \hline
		pang\_movie & 0.2650 & 0.2651 & 0.2641 &0.2641 \\ \hline
		sanders & 0.3072 & 0.314 & 0.2976 & 0.2976\\ \hline
		sentistrength\_bbc & 0.5090 &0.5110& 0.5090 & 0.5090\\ \hline
		sentistrength\_digg & 0.4903 &0.4847 & 0.4791 & 0.4801 \\ \hline
		sentistrength\_myspace & 0.3535 & 0.3468 & 0.3535 & 0.3535\\ \hline
		sentistrength\_rw & 0.4072 & 0.4092 & 0.4111 & 0.4111\\ \hline
		sentistrength\_twitter & 0.3777 & 0.3755 & 0.3685 & 0.3685\\ \hline
		sentistrength\_youtube & 0.3774 & 0.3725 & 0.3713 & 0.3713\\ \hline
		stanford\_tweets & 0.3621 & 0.3566 & 0.3232 & 0.3232\\ \hline
		tweet\_semevaltest & 0.3470 & 0.3468 & 0.3447 & 0.3448\\ \hline
		vader\_amazon & 0.5885 & 0.5890 &0.5626 & 0.5626\\ \hline
		vader\_movie &0.2315 & 0.2315 & 0.2318 & 0.2318\\ \hline
		vader\_twitter & 0.1795 & 0.1781 & 0.1774 & 0.1771\\ \hline
		vader\_nyt & 0.6657& 0.6711 & 0.6524 & 0.6522\\ \hline
		Error medio & 0.3937 & 0.3921 & 0.3836 & 0.3836 \\ \hline 
		\textbf{all\_datasets} &0.4911&0.4776&0.4697&0.4697\\ \hline
	\end{tabular} 
}
\caption{Tabla resumen de los errores medios de clasificación consiguiendo aplicando algoritmos evolutivos. Tablas completas en anexos.}
\label{table:evolutivos}
\end{table}

En la primera tabla se observa que los algoritmos basados en evolución diferencial obtienen mejor error medio. Esto muestra cómo un algoritmo que enfatiza la mutación y, por lo tanto, la exploración produce mejores resultados, reflejo claro de que en nuestro problema es más conveniente emplear algoritmos que exploren diferentes espacios de búsqueda, ya que el dominio de nuestro problema, al tratarse de la optimización de pesos reales, es infinito.

\newpage
\begin{table} [H]
	\scalebox{0.6}{
		\begin{tabular}{|l|b{1.7cm}b{1.8cm}b{1.7cm}llll|}
			\hline 
			\textbf{Dataset} & \textbf{\specialcell{AGG-CA\\Trunc.}} & \textbf{\specialcell{AGG-CA\\(SA)}}& \textbf{\specialcell{AGE-CA\\Trunc.}} & \textbf{AGE-CA (SA)} & \textbf{AM(10,1.0)} & \textbf{AM(10,0.1)} & \textbf{AM(10,0.1MEJ)} \\ 
			\hline 
			aisopos\_ntua & 0.3280 & 0.3260 & 0.3380 & 0.3280 &\cellcolor{greentable} 0.3220 & 0.3280 & \cellcolor{greentable}0.3220\\ \hline
			debate & 0.5281 & 0.5259 & 0.5235 & 0.5275&0.5238 &0.5222&0.5265 \\ \hline
			english\_dailabor & 0.2514 &0.2233 & 0.2304 & 0.2294&\cellcolor{greentable}0.2220&0.2254&0.2241\\ \hline
			nikolaos\_ted & 0.5256 & 0.4827& 0.5113 & 0.5244&0.4875&\cellcolor{greentable}0.4815&0.4875 \\ \hline
			pang\_movie & 0.2651 &\cellcolor{greentable}0.2640 & 0.2651 &0.2651 &0.2641 &0.2641&0.2641 \\ \hline
			sanders & 0.3110 & 0.2961 & 0.3192 & 0.3002 & \cellcolor{greentable}0.2953 & 0.2979 & 0.2976\\ \hline
			sentistrength\_bbc & 0.5290 &0.5200& 0.5050 & 0.4970&\cellcolor{greentable}0.4900 &0.5070&0.4960\\ \hline
			sentistrength\_digg & 0.4875 &0.4800 & 0.4837 & 0.4856&0.4800&0.4773&\cellcolor{greentable}0.4745  \\ \hline
			sentistrength\_myspace &\cellcolor{greentable} 0.3448 & 0.3545 & 0.3477 & 0.3545&0.3583&0.3525&0.3746\\ \hline
			sentistrength\_rw & 0.4120 & 0.4101 & 0.4149 & \cellcolor{greentable}0.4063&0.4149 &0.4101&0.4111\\ \hline
			sentistrength\_twitter & 0.3767 & 0.3696 & 0.3746 & 0.3744&\cellcolor{greentable}0.3661&0.3682&0.3673 \\ \hline
			sentistrength\_youtube & 0.3760 & 0.3686 & 0.3727 &\cellcolor{greentable} 0.3675&0.3701&0.3704&0.3704\\ \hline
			stanford\_tweets & 0.3510 &\cellcolor{greentable} 0.3204 & 0.3622 & 0.3454&0.3259&0.3232&0.3343\\ \hline
			tweet\_semevaltest & 0.3527 & 0.3475 & 0.3488 & 0.3432& \cellcolor{greentable}0.3361&0.3447&0.3429 \\ \hline
			vader\_amazon & 0.5884 &\cellcolor{greentable} 0.5620 &0.5801 & 0.5849&0.5860&0.5639&0.5742\\ \hline
			vader\_movie & 0.2315 & 0.2314 & 0.2318  & 0.2315&\cellcolor{greentable}0.2306&0.2318&0.2310\\ \hline
			vader\_twitter & 0.1781 & 0.1790 & 0.1774 & \cellcolor{greentable}0.1750&0.1781&0.1779&0.1824 \\ \hline
			vader\_nyt & 0.6651& 0.6497 & 0.6663 & 0.6651&\cellcolor{greentable}0.6364&0.6493&0.6393\\ \hline
			Error medio & 0.3946 & 0.3839 & 0.3918 & 0.3892 & 0.3826 & 0.3831 & 0.3844 \\ \hline
			\textbf{all\_datasets} &0.4836&0.4691&0.4761&0.4810&0.4703&0.4700&0.4712\\ \hline
		\end{tabular} 
	}
	\caption{Tabla con las tasas de error de clasificación medias de los algoritmos genéticos con variaciones. Tablas completas en anexos.}
	\label{table:mejoras}
\end{table}

Los algoritmos que mejor resultados nos ofrecen son los algoritmos meméticos, implementados sobre un genético con esquema generacional. El esquema generacional, al cambiar la población completa en cada iteración, favorece a la exploración. Al incluirle búsqueda local hacemos que se mantenga un equilibrio entre la explotación y la exploración, siendo este equilibrio el que permite obtener soluciones de mayor calidad.
\newpage
\begin{table} [H]
	\scalebox{0.6}{
		\begin{tabular}{|c|cccc|}
			\hline 
			\textbf{Dataset} & \textbf{Polaridad media} &\textbf{Combinación proneutral}  & \textbf{Mejor error individual} & \textbf{Mejor error evolutivos}\\ 
			\hline 
			aisopos\_ntua &0.378 &0.394 & 0.368 & \cellcolor{greentable} 0.322\\ \hline
			debate & 0.522  & 0.529  & 0.528 & \cellcolor{greentable} 0.5213\\ \hline
			english\_dailabor & 0.243  & 0.226 & 0.313 & 0.2220 \cellcolor{greentable}\\ \hline
			nikolaos\_ted & 0.522 & 0.549 &\cellcolor{greentable} 0.383 & 0.4815\\ \hline
			pang\_movie & 0.586 & 0.626 & 0.265 & \cellcolor{greentable} 0.264\\ \hline
			sanders & 0.321 & 0.308 & 0.397 & \cellcolor{greentable} 0.2953\\ \hline
			sentistrength\_bbc & 0.573 &0.573 &\cellcolor{greentable} 0.415 & 0.49\\ \hline
			sentistrength\_digg & 0.4977 &0.518 & 0.457 &  \cellcolor{greentable}0.4745 \\ \hline
			sentistrength\_myspace & 0.418 & 0.446 &\cellcolor{greentable} 0.283 & 0.3448\\ \hline
			sentistrength\_rw & 0.428 & 0.455 & 0.434 & \cellcolor{greentable}0.4063\\ \hline
			sentistrength\_twitter & 0.386 & 0.395 & 0.391 & \cellcolor{greentable} 0.3661\\ \hline
			sentistrength\_youtube & 0.382 & 0.402 & 0.388 & \cellcolor{greentable}0.3675\\ \hline
			stanford\_tweets & 0.459 & 0.499 &\cellcolor{greentable} 0.267 & 0.3204\\ \hline
			tweet\_semevaltest & 0.338 & 0.340 & 0.375 & \cellcolor{greentable} 0.3361\\ \hline
			vader\_amazon & 0.615 & 0.643 &\cellcolor{greentable} 0.419 & 0.562\\ \hline
			vader\_movie & 0.585 & 0.624 &\cellcolor{greentable} 0.231 & \cellcolor{greentable} 0.231\\ \hline
			vader\_twitter & 0.366 & 0.435 & 0.178 & \cellcolor{greentable} 0.175\\ \hline
			vader\_nyt & 0.6867& 0.719 &\cellcolor{greentable} 0.439 & 0.6364\\ \hline
			Error medio & 0.461 & 0.482 & \cellcolor{greentable} 0.363 & 0.378 \\ \hline
		\end{tabular} 
	}
	\caption{Tabla resumen de los errores medios de los ensamblados y los mejores errores de las herramientas individuales y los algoritmos evolutivos}
	\label{table:combinacionesygeneticos}
\end{table}

En la Tabla \ref{table:combinacionesygeneticos} vemos una comparación de los mejores resultados obtenidos con los algoritmos evolutivos con las tasas de error de los operadores de ensamblado de herramientas y la mejor tasa de error de las herramientas. Se aprecia como los algoritmos evolutivos mejoran hasta en 11 conjuntos de datos la mejor tasa de error de las obtenidas hasta el momento. Sin embargo, en 6 alguna de las herramientas de forma individual obtiene mejores resultados (en algunos \textit{datasets} como en vader\_nyt por bastante diferencia). En vader\_movie hay un empate entre las herramientas de forma individual y los algoritmos evolutivos.

El ensamblado mediante pesos obtenidos con algoritmos evolutivos mejora en todos los conjuntos de comentarios los ensamblados propuestos en este proyecto. Sin embargo, en vader\_nyt con ningún ensamblado se obtienen unos resultados aceptables, pero también hay que tener en cuenta que casi todas las herramientas en este conjunto de datos obtiene tasas de error mayores del 50\%.

\subsection{Comportamiento algoritmos evolutivos}
Como se ha observado previamente, la exploración de los algoritmos evolutivos puede ser primordial, sobre todo en problemas como el nuestro en el que la optimización es de valores reales, siendo así el dominio infinito. Como se observa en las Figuras \ref{fig:age} y \ref{fig:agg}, los 2 algoritmos se comportan de forma distinta a la hora de converger hacia la mejor solución. AGG converge hacia la mejor solución mucho más rápido que AGE, consecuencia directa de que AGG en cada iteración cambie toda su población, añadiendo así una mayor exploración, y AGE solo cambie 2 cromosomas por iteración.
\begin{figure} [H]
	\centering
	\includegraphics[width=0.7\linewidth]{imagenes/AGE}
	\caption{Evolución del ECM de la mejor solución en AGE}
	\label{fig:age}
\end{figure}

\begin{figure} [H]
	\centering
	\includegraphics[width=0.7\linewidth]{imagenes/AGG}
	\caption{Evolución del ECM de la mejor solución en AGG}
	\label{fig:agg}
\end{figure}

Otro aspecto interesante es la asignación de pesos por parte de los algoritmos, que deberían tender a asignar mayor peso a la mejor herramienta para ese \textit{dataset}. De esta manera arreglaríamos el hecho de que con los otros ensamblados propuestos se tengan en cuenta por igual herramientas que tienen un mal funcionamiento en determinados conjuntos de comentarios. En las siguientes figuras se observan los pesos asignados a las distintas herramientas en determinados conjuntos de comentarios empleando AGG-CA, donde:
\begin{itemize}
	\item La herramienta 1 es Azure.
	\item La herramienta 2 es Bing.
	\item La herramienta 3 es CoreNLP.
	\item La herramienta 4 es MeaningCloud.
	\item La herramienta 5 es SentiStrength
	\item La herramienta 6 es Syuzhet
	\item La herramienta 7 es VADER.
\end{itemize}
\begin{figure} [H]
	\centering
	\includegraphics[width=0.7\linewidth]{imagenes/vader_twitter_sol1}
	\caption{Solución dada por una de las iteraciones en AGG-CA para vader\_twitter}
	\label{fig:vadertwittersol1}
\end{figure}

\begin{figure} [H]
	\centering
	\includegraphics[width=0.7\linewidth]{imagenes/vadertwittersol2}
	\caption{Solución dada por una de las iteraciones en AGG-CA para vader\_twitter}
	\label{fig:vadertwittersol2}
\end{figure}

En las Figuras \ref{fig:vadertwittersol1} y \ref{fig:vadertwittersol2} se aprecia cómo ambas soluciones son bastante diferentes. En el \textit{dataset} de vader\_twitter casi todas las herramientas, a excepción de CoreNLP tienen buen comportamiento. Sin embargo en la primera solución sorprendentemente CoreNLP obtiene algo de peso. En la segunda este aspecto se arregla ya que CoreNLP no obtiene casi nada de peso y este, en su mayoría se reparte entre VADER y MeaningCloud.

Un conjunto de datos en el que es interesante también estudiar la asignación de pesos es en english\_dailabor, ya que aquí se obtiene una tasa de error de hasta casi un 10\% menor que en las herramientas por individual. 

\newpage
\begin{figure} [H]
	\centering
	\includegraphics[width=0.7\linewidth]{imagenes/englishdailaborsol1}
	\caption{Solución obtenida en una de las iteraciones con AGG-CA para english\_dailabor}
	\label{fig:englishdailaborsol1}
\end{figure}

\begin{figure} [H]
	\centering
	\includegraphics[width=0.7\linewidth]{imagenes/englishdailaborsol2}
	\caption{Solución obtenida en una de las iteraciones con AGG-CA para english\_dailabor}
	\label{fig:englishdailaborsol2}
\end{figure}

En las Figuras \ref{fig:englishdailaborsol1} y \ref{fig:englishdailaborsol2} se aprecia cómo ambas soluciones son similares y destacan principalmente por un reparto de los pesos con algo de equilibrio. Al conseguir una tasa de error tan mejorada con respecto a la conseguida con la mejor herramienta, resulta necesario que los pesos se repartan entre otras herramientas para, digamos informalmente, reparar o mejorar los resultados presentados por la mejor herramienta, en este caso SentiStrength.

Donde mejor se aprecia la adaptación de los pesos es en el conjunto de comentarios vader\_movie, en el que todas las herramientas menos CoreNLP, que obtiene una de sus mejores tasas de error en este \textit{dataset}, obtienen resultados muy malos. Como se observa en la Figura \ref{fig:vadermoviesol1}, CoreNLP obtiene más de un 90\% del peso, señalándonos claramente que las demás herramientas ofrecen una calidad muy pobre. Además, en la Figura \ref{fig:vadermoviesol2} se aprecia la solución dada por AGG-CA con truncamiento, donde pone todos los pesos a 0 menos CoreNLP, que obtiene todo el peso de relevancia.
\begin{figure} [H]
	\centering
	\includegraphics[width=0.7\linewidth]{imagenes/vadermoviesol1}
	\caption{Solución obtenida en una de las iteraciones en AGG-CA para vader\_movie}
	\label{fig:vadermoviesol1}
\end{figure}

\newpage
\begin{figure} [H]
	\centering
	\includegraphics[width=0.7\linewidth]{imagenes/vadermoviesol2}
	\caption{Solución obtenida en una de las iteraciones en AGG-CA con truncamiento para vader\_movie}
	\label{fig:vadermoviesol2}
\end{figure}
